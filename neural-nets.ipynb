{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a bit of fun. We see that `torch` has trainable tensors like `tensorflow` but is flexible like `numpy`. Now we combine these traits to build a basic feedforward neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this, we use the `torch.nn` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![lecun mnist architecture](https://pytorch.org/tutorials/_images/mnist.png)\n",
    "We will build this thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__() # initializes from the superclass\n",
    "        ## our net will be as in the image above\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5) # arguments are 1 input channel, 6 output channels, 5x5 kernel size\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # forward propagation\n",
    "        # note that in the architecture we have subsampling layers. this is implemented as 2d maxpools\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2,2)) #2x2 max pooling layer\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), (2,2))\n",
    "        # now we're at the fc layers\n",
    "        # here, x is a 2d image, but we gotta flatten it for input into the fc layers\n",
    "        x = x.view(-1, self.flatten(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def flatten(self, x):\n",
    "        shape = x.size()[1:]\n",
    "        dims = 1\n",
    "        for d in shape:\n",
    "            dims *= d\n",
    "        return dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we didn't define a backpropagation function. That's because `torch` does it for you with its `autograd` magic.\n",
    "\n",
    "We can rip out of the parameters for the network using `.parameters()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "params = list(model.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size()) # params for first convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1150,  0.1102, -0.0547,  0.0554, -0.0507, -0.0807,  0.0716, -0.0418,\n",
      "          0.0664,  0.0718]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = model(input) # syntactic sugar for model.forward(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Why did we take a tensor of dimension $(1,1,32,32)$ as input, as opposed to just $(1,32,32)$? `torch.nn` only takes in *minibatches* of data, so the first value refers to how many samples there are.\n",
    "\n",
    "To actually train this, we need some objective/loss function to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9534, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "target = torch.randn(10)\n",
    "target = target.view(1,-1)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(out, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to backprop. We first clear out all the gradient buffers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "None\n",
      "conv1.bias.grad after backward\n",
      "tensor([ 0.0066, -0.0103, -0.0038,  0.0122, -0.0099,  0.0366])\n"
     ]
    }
   ],
   "source": [
    "print('conv1.bias.grad before backward')\n",
    "print(model.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(model.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loads up all the gradients coming from the loss function. Now we can act upon these gradients to do SGD. The thing is that we don't need to rely on custom built optimizers like Adam or RMSProp. We just do it ourselves, with the gradients and parameters in full view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "for f in model.parameters():\n",
    "    # perform SGD\n",
    "    f.data.sub_(f.grad.data * learning_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
